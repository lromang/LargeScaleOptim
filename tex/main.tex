\documentclass{book}
%Para que los números de la tabla de contenidos
%sean ligas a las distintas secciones.
\usepackage[linktocpage]{hyperref}
%Apendices
\usepackage{appendix}
\usepackage{chngcntr}
\usepackage{etoolbox}
%Para rellenar algunas secciones.
\usepackage{lipsum}
%Poder insertar imágenes
\usepackage{graphicx}
%Notación matemática
\usepackage{amsmath,amsthm,epsfig,epstopdf,amsfonts}
\usepackage{bbm}
%Ambiente para teoremas, lemmas, definiciones
%Idioma y encoding
\usepackage[spanish, mexico]{babel}
\usepackage[utf8]{inputenc}
%
\theoremstyle{plain}
\newtheorem{thm}{Teorema}[section]
\newtheorem{lem}[thm]{Lema}
\newtheorem{prop}[thm]{Proposición}
\newtheorem*{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{defn}{Definición}[section]
\newtheorem{conj}{Conjetura}[section]
\newtheorem{exmp}{Ejemplo}[section]
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
% Otros paquetes para notación matemática
\usepackage{mathtools}
\usepackage{amssymb}
%Tablas de color
\usepackage[table]{xcolor}
%Secciones con otra estructura aparte de la
%de la página
\usepackage{float}
\selectlanguage{spanish}
%Pseudo código
\usepackage[Algoritmo]{algorithm}
\usepackage[noend]{algpseudocode}
%Citas
\usepackage{cite}
%Formato especial para ocupar toda la hoja
\usepackage{fullpage}
%Margenes de parrafos
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist
%\usepackage[a4paper]{geometry}
%Secciones con su propio ambiente
%para código por ejemplo
\usepackage{listings}
\usepackage{courier}
\usepackage{chngcntr}
\counterwithin{table}{section}
\counterwithin{figure}{section}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{minitoc}
\usepackage{subcaption}
\usepackage{multirow}
%Texto
\renewcommand{\baselinestretch}{1.5} 
\addtolength{\skip\footins}{8mm}

%Colores
\usepackage{xcolor}
\usepackage{textcomp}
%-----Epigrafos---------
\usepackage{epigraph}
%-----------------------
%----Títulos de capítulo fancy
\usepackage{titlesec, blindtext, color}
\usepackage{titlesec, blindtext, color}
\definecolor{gray75}{gray}{0.75}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{\vline}\hsp}{0pt}{\Huge\bfseries}
%Encabezado en cada página
%-----------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
%\usepackage[margin=4.4cm,headheight=35pt,showframe]{geometry}
\fancyhf{}
\rhead{\rightmark}
\lhead{Proyecto de Tesis}
\rfoot{\thepage}
\usepackage[margin=3cm,headsep=1cm,headheight=2cm]{geometry}
%-------------------------
\usepackage{fancyvrb}
\definecolor{listinggray}{gray}{0.95}
\definecolor{lbcolor}{rgb}{0.95,0.95,0.95}
\lstset{
        backgroundcolor=\color{lbcolor},
        tabsize=4,
        language=R,
    basicstyle=\scriptsize,
    upquote=true,
    aboveskip={0.5\baselineskip},
    columns=fixed,
    showstringspaces=false,
    extendedchars=true,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=none,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
    basicstyle=\ttfamily\small,
    breaklines=true,
    numbers=left,
    numberstyle=\footnotesize,
    stepnumber=1,
    numbersep=0.5cm,
    xleftmargin=0.2cm,
    xrightmargin=0.3cm,
    frame=tlbr,
    framesep=5pt,
    framerule=0pt,
}

\newcommand{\eqdef}{\overset{\mathrm{def}}{=\joinrel=}}

\addto\captionsspanish{% Replace "english" with the language you use
  \renewcommand{\contentsname}%
    {Contenido}%
}
%--------pseudocódigo

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
%------------------------Abstracts------------------------
\makeatletter
\if@titlepage
  \newenvironment{abstract}{%
      \titlepage
      \null\vfil
      \@beginparpenalty\@lowpenalty
      \begin{center}%
        \bfseries \abstractname
        \@endparpenalty\@M
      \end{center}}%
     {\par\vfil\null\endtitlepage}
\else
  \newenvironment{abstract}{%
      \if@twocolumn
        \section*{\abstractname}%
      \else
        \small
        \begin{center}%
          {\bfseries \abstractname\vspace{-.5em}\vspace{\z@}}%
        \end{center}%
        \quotation
      \fi}
      {\if@twocolumn\else\endquotation\fi}
\fi
\makeatother
%-----------------------No cambio de página----------------
\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}
%----------------------------------------------------------


\setlength{\parskip}{1cm}
\title{Métodos estocásticos de segundo orden con aplicación a problemas de aprendizaje de máquina}
\author{Luis Manuel Román García}
\date{\today}

% ambiente de apendice
\AtBeginEnvironment{subappendices}{%
\chapter*{Appendix}
\addcontentsline{toc}{chapter}{Appendices}
\counterwithin{figure}{section}
\counterwithin{table}{section}
}

\begin{document}
\pagenumbering{gobble}
\maketitle
\VerbatimFootnotes


\begin{absolutelynopagebreak}
\begin{abstract}
En este documento se estudia el desempeño de dos algorítmos de optimización \emph{LBFGS} y \emph{Newton Truncado} cuando se utiliza información incompleta de segundo orden, es decir, cuando la matriz hessiana es aproximada por medio de una muestra de los datos. La idea es aprovechar la naturaleza estocástica de las funciones objetivo que a menudo surgen en aplicaciones del campo de aprendizaje de máquina. De igual forma, se exploran sus propiedades teóricas de convergencia y se pone a prueba su precisión con una tarea de clasificación de mensajes de voz.
\end{abstract}
\end{absolutelynopagebreak}

%--------------------
% Tabla de contenidos
%\begin{absolutelynopagebreak}
\addtocontents{toc}{\protect\sloppy}
\tableofcontents
\addtocontents{toc}{~\hfill\textbf{Página}\par}
%\end{absolutelynopagebreak}
%--------------------
\pagenumbering{roman}


\chapter{Aprendizaje a gran escala}


\epigraph{Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.}{\textit{Bousquet \\ 2008}}

\newpage

\section{Estadística y aprendizaje}

Tal y como Vapnik menciona en \cite{VAPNIK1}, el paradígma del aprendizaje estadístico nace en la década de los 60's gracias a que el incremento en la capacidad computacional permitió llevar a cabo análisis multidimensionales de fenómenos naturales, antes impracticables. Estos análisis demostraron que, en su mayoría, los modelos de baja dimensionalidad eran poco precisos e incluso complteamente erróneos. Se comenzaron a cuestionar los planteamientos clásicos de la estadística y se reconcoció que la mayor parte de los supuestos que la sustentan no se cumplen cuando la complejidad del fenómeno bajo estudio aumenta. Derivado de esto, surgieron dos maneras distintas de abordar el problema clásico de la inferencia estadística:

\begin{enumerate}
\item El particular \emph{paramétrico}: busca generara métodos de estadística inferencial  simples que puedan ser utilizados para resolver problemas de la vida diaria.
\item El general \emph{no paramétrico}: busca encontrar un método inductivo para cualquier problema de inferencia estadística.
\end{enumerate}

Ambas filosofías difieren en el número de supuestos que se plantean y en la cantidad de conocimiento que se le atribuye al usuario. De esta manera, el primer paradigma parte del hecho de que:

\begin{changemargin}{1.5cm}{1.5cm}
\emph{El investigador conoce el problema que analiza y tiene conocimiento de las causas físicas detrás de la estocasticidad de las observaciones.}
\end{changemargin}

Los principales supuestos que sustentan este planteamiento son los siguientes:

\begin{itemize}
\item La mayor parte de las relaciones funcionales subyacentes en los datos pueden ser aproximadas satisfactoriamente por una función lineal en sus parámetros.
\item La normalidad es la regla subyacente en la mayor parte de los fenómenos de la naturaleza.
\item La máxima verosimilitud es una buena herramienta inductiva para la estimación de parámetros.
\end{itemize}

Pese a que estos supuestos son razonables, ciertas fallas fundamentales obligaron a los investigadores y teóricos a buscar alternativas. Es curioso que dichas fallas se descubrieron en un corto periodo de tiempo: 1955-1961; Mismo periodo en el que F. Rosenblatt descubrió el primer modelo de aprendizaje: \emph{El Perceptrón} (1958).

La impractibilidad del primer supuesto quedó evidenciada conforme al progreso científico. Dado que las indagaciones teóricas sobre procesos naturales daban por resultado interacciones complejas entre distintos fenómenos, los modelos requeridos para explicarlos fueron exigiendo un mayor número  de variables y flexibilidad (interacciones no lineales). En consecuencia, volviéndose más complejos. Sin embargo, y como R. Bellman demostró en 1961, la cantidad de datos requerida para ajustar un modelo crece de manera exponencial conforme al número de parámetros. Esto hace que sea computacionalmente intratable ajustar modelos lineales (en los parámetros) arbitrariamente complejos. 

Por ejemplo, siguiendo las líneas de \cite{VAPNIK1}, de acuerdo con el teorema de aproximación de Weierstrass. \footnote{\begin{thm}Si $f$ es una función continua definida sobre un compacto $k$, entonces para todo $\epsilon > 0, \exists N_0\in\mathbb{N}$ tal que $\forall n\geq N_0$ se cumple que:
\begin{equation*}
  \begin{split}
    \|f(x) - P_n(x)\|_1 &< \epsilon\quad\forall x\in K
  \end{split}
\end{equation*}
Donde $P_n$ es un polinómio de grado n.
\end{thm}} Toda función continua de $n$ variables definida sobre el cubo unitario puede ser aproximada con un error arbitrariamente pequeño por polinomios. Sin embargo, si la función tiene únicamente $s<n$ derivadas, entonces con polinomios de grado $N$ sólo se puede aproximar con una precisión de orden $\large{O}(N^{\frac{s}{n}})$. Esto nos dice que si $s$ es pequeña, se requerirá un incremento exponencial en $N$ (el número de observaciones) para compensar incrementos en $n$ (la complejidad del modelo).

El segundo supuesto, comenzó a verse en problemas desde unos años antes, pero fue Tukey en 1960 con\cite{TUKEY} quien puso fin a la universalidad del supuesto de normalidad al analizar muestras bajo distribuciones contaminadas. La principal interrogante consistía en determinar si los métodos de estimación que exhibian propiedades óptimas, tales como eficiencia,  bajo ciertos supuestos, preservaban sus propiedades una vez que dichos supestos eran relajados. El experimento que Tukey llevó a cabo fue introducir cierto número de valores dentro de una muestra proveniente de una distribución normal. Los valores introducidos provenian de una distribución normal con el mismo parámetro de locación pero un parámetro de escala tres veces mayor. Los resultados demostraron que bajo mezclas cási imperceptibles (.008 de los datos provenientes de la muestra con mayor desviación estandar), las propiedades de eficiencia de los estimadores (en este caso el estimador de la varianza) se pierden por completo. En otras palabras, desviaciones nímias del supuesto de normalidad puede deteriorar en gran medida la confiabilidad de las estimaciones.  

En cuanto al tercer supuesto, en 1955, Stein mostró en \cite{STEIN1} que el estimador de máxima verosimilitud para una muestra unitaria proveniente de una normal multivariada con número de dimensiones $n\geq 3$ es un estimador inadmisible \footnote{\begin{defn}Decimos que un estimador $\hat{\theta}$ es \textbf{mejor que} un estimador $\theta^*$ si $\forall\mu,R(\mu,\hat{\theta})\leq R(\mu,\theta^*)$ y $\exists\mu'$ tal que $R(\mu,\hat{\theta})<R(\mu,\theta^*)$. Un estimador $\hat{\theta}$ es \textbf{admisible} si no existe un estimador $\theta^*$ \textbf{mejor que} $\hat{\theta}$, donde $R$ es la función de error definida en el capítulo 2. Un estimador es inadmisible si no es admisible.\end{defn}}. Este resultado fue de tal importancia que según palabras de Bradley Efron:

\begin{changemargin}{1.5cm}{1.5cm}
\emph{Socavó siglo y medio de teoría de estimación, yendo atrás hasta Karl Friederich Gauss y Adrein Marie Legendre.}
\end{changemargin}

Para una discusión muy intuitiva sobre este resultado y sobre el estimador propuesto por James y Stein en \cite{STEIN2}, ver \cite{EFRON}. 

Las fallas en los supuestos fundamentales del paradígma clásico de la inferencia estadística urgieron el desarrollo de técnicas más generales que descansaran en un menor número de supuestos. El paradígma \emph{general no paramétrico} resulto ser una alternativa prometedora. Este enfoque para el problema de la inferéncia estadística  es mucho más conservador en cuanto al grado de conocimiento del problema y a las hipótesis subyacentes. A saber, este paradigma sostiene que:

\begin{changemargin}{1.5cm}{1.5cm}
\emph{El investigador no tiene conocimiento confiable a priori de la regla estadística subyacente en el fenómeno bajo observación o de la función que uno quisiera aproximar.}
\end{changemargin}

Bajo estas líneas, es necesario preguntarse en que medida el investigador puede hacer uso de los datos a su disposición para hacer inferencia. Más aún, queda determinar que tan adecuado es este método y si es capaz de arrojar resultados adecuados para la tarea propuesta. En esta dirección surgen dos resultados clave, el primero dice que tiene sentido utilizar los datos que el investigador tiene a su disposición para aproximar la distribución subyacente del fenómeno bajo estudio y el segundo habla de la presición asintótica de dicha aproximación. El primer resultado es el teorema  Gilvenko-Cantelli:

\bigskip

\begin{thm}
Sean $X_1,X_2,\dots,X_n$ variables aleatorias iid con una distribución $F$ con valores en $\mathbb{R}$. Entonces se cumple que
\begin{equation*}
\begin{split}
\displaystyle\sup_{x\in\mathbb{R}}\bigg\|\frac{1}{n}\displaystyle\sum_{i=1}^n\mathbb{I}_{x_i\leq x}-F_{x}(x_i)\bigg\| &\xrightarrow[n\rightarrow\infty]{a.s} 0
\end{split}
\end{equation*}
\end{thm}

Es decir, la distribución empiríca obtenida de promediar los valores observados en la muestra, converge cási seguramente a la distribución de probabilidad real. Y el segundo se conoce como la desigualdad de Dvoretzky-Kiefer-Wolfowitz \cite{MASSART}.

\bigskip

\begin{thm}
Sean $X_1,X_2,\dots,X_n$ variables aleatorias iid con una distribución $F$ con valores en $\mathbb{R}$ y $\lambda\in\mathbb{R}^+$. Entonces se cumple que
\begin{equation*}
\begin{split}
P\bigg\{\sqrt{n}\displaystyle\sup_{x\in\mathbb{R}}\bigg\|\frac{1}{n}\displaystyle\sum_{i=1}^n\mathbb{I}_{x_i\leq x}-F_{x}(x_i)\bigg\| > \lambda \bigg\}& \leq 2e^{-2\lambda^2}
\end{split}
\end{equation*}
\end{thm}

Como puede observarse, la eficiencia del paradígma \emph{general de la estadística} depende de la abundancia de datos \footnote{La dependencia de ambos límites del tamaño de la muestra es evidente.}, no obstante, hay una serie de complicaciones empíricas de las cuales la exposición teórica hasta el momento no ha dado cuenta. En las siguientes secciones aterrizaremos más la problemática en cuestión, daremos una breve introducción al problema principal del aprendizaje de máquina, exploraremos las principales complicaciones que surgen a la hora de implementar dichos algoritmos y en particular nos enfocaremos en el área de optimización y en las diversas técnicas que han surgido para solventar dichas dificultades. 

\section{El problema del aprendizaje}

El objetivo del aprendizaje estadístico es el diseño e implementación de algoritmos que sean capaces de \emph{aprender} las interacciones existentes entre un conjunto de datos y un conjunto de respuestas y generalizar dichas percepciones para observaciones futuras. En este sentido, se supondra que existe una \emph{regla} subyacente que rige la relación entre las variables observables y las respuestas \footnote{Sin perdida de generalidad, siguiendo las líneas de \cite{VAPNIK1}, nos centraremos en el contexto de clasificación binaria. Esto se debe a que es el escenario más simple de analizar, sin que esta simplicidad sacrifique generalidad.}. De manera concreta, supondremos un conjunto arbitrario de observaciones $X=\{x_1,x_2,\dots,x_n\}$ tal que $x_i \stackrel{iid}{\sim} D,\quad\forall x_i\in X$ y un conjunto de respuestas $Y=\{y_1, y_2, \dots,y_n\};y_i\in \{0,1\}$. Tal que existe una función $c:X\rightarrow Y$ que rige la manera en la que cada elemento $x_i$ está relacionado con una respuesta $y_i$. El problema principal del aprendizaje estadístico se reduce entonces a encontrar de manera \emph{eficiente} una función o hipótesis $h:S\subset X\rightarrow Y$ donde $S\sim D^n$ tal que \emph{probablemente} estime a $c$ de manera \emph{razonablemente correcta}.

Del párrafo anterior se desprenden varias interrogantes: ¿Qué significa que un algoritmo aprenda con \emph{alta probabilidad} una representación \emph{razonablemente correcta}?, ¿Qué significa que lo haga eficientemente? y ¿Cuándo es esto posible?. Con estas interrogantes en mente, resulta conveniente entonces definir un criterio bajo el cual se puede evaluar el desempeño de un determinado \emph{algoritmo de aprendizaje} con tal de determinar si cumple las características de lo que puede llegar a ser \emph{aprendible}:

\bigskip\bigskip


\begin{defn}\label{eq:def_gen_err}
\textbf{Error de generalización}\\
  Dado un par de conjuntos $X$, $Y$ donde $x_i \stackrel{iid}{\sim} D,\forall x_i\in X$, una función $c:X\rightarrow Y$ y una hipótesis $h:X\rightarrow Y$. El error de generalización de $h$ está dado por:
\begin{equation*}
R(h)=P_{x\sim D}\{x\in X| h(x)\neq c(x)\}=E\bigg(\mathbbm{1}_{\{x\in X| h(x)\neq c(x)\}}\bigg)
\end{equation*}
\end{defn}

Como puede observarse, el error de generalización no es otra cosa que la \emph{medida} del espacio en el cual la hipótesis aprendida $h$ diferirá de la relación intrínseca $c$. De esta forma podemos determinar una tolerancia $\epsilon > 0$ y llamar a todo algoritmo que genere hipótesis $h$ tal que $R(h)\leq\epsilon$ como razonablemente correcto o \emph{aproximadamente} correcto. Además de esto, también nos da una medida para comparar el desempeño de un algoritmo con respecto a otro. No obstante lo adecuado de esta definición, una dificultad evidente al querer evaluar el desempeño de un modelo de manera empírica es que ni la distribución $D$, ni la función $c$ son conocidas. Más aún, es probable, salvo casos triviales, que nisiquiera se cuente con $X$ si no con una muestra finita $S \subset X$. En consecuencia, se define el \emph{Error empírico} como un aproximador insesgado de $R(h)$ de la siguiente manera:
\bigskip

\begin{defn}\label{eq:def_emp_err}
\textbf{Error empírico}\\  Dados un par de conjuntos $X$, $Y$ donde $x_i \stackrel{iid}{\sim} D,\forall x_i\in X$, una muestra $S\subset X$ con $S\sim D^n$ y una hipótesis $h:S\subset X\rightarrow Y$. El error de empírico de $h$ está dado por\footnote{El que este estimador sea insesgado es una consecuencia directa de la ley de los grandes números.\cite{BERNOULLI}}:
\begin{equation*}
\hat{R(h)}=\frac{1}{n}\displaystyle\sum_{i=1}^n\mathbbm{1}_{\{x\in S| h(x)\neq c(x)\}}
\end{equation*}
\end{defn}

Para aligerar la notación, en lo siguiente se denotará la función de pérdida como $l(h,c)$. En este contexto, y con tal de esclarecer las nociones de estimación \emph{probablemente aproximadamente correcta} para una regla $c$, se buscará caracterizar el comportamiento de:
\begin{defn}\label{eq:def_emp_err_g}
\begin{equation*}
\begin{split}
\hat{R(h)}=\frac{1}{n}\displaystyle\sum_{i=i}^n l(h,c)
\end{split}
\end{equation*}
\end{defn}

Dada una familia de hipótesis $H$. Por lo tanto diremos que una regla $c$ es \emph{Probablemente aproximadamente correctamente aprendible} o \emph{PAC} aprendible por H si:

\begin{defn}\label{eq:pac_aprendible}
\textbf{PAC Aprendible}\\  Dados un par de conjuntos $X$, $Y$ donde $x_i \stackrel{iid}{\sim} D,\forall x_i\in X$ existe una hipótesis $h_{n^{*}}:S\subset X\rightarrow Y\in H$. Tal que $\forall \epsilon,\delta > 0$ $\exists n^{*} > 0 \in \mathbb{N}$ con $|S|\geq n^{*}$ que cumple que:
\begin{equation*}
P\{\hat{R(h_{n^{*}})}\geq R(h) + \epsilon\} < \delta
\end{equation*}
\end{defn}

En \cite{VALIANT}, Valiant agregó la característica que para que una familia de \emph{reglas} o \emph{conceptos} sean aprendibles, estos deben poder serlo en tiempo polinomial con respecto al tamaño de los datos, de ahí la noción de eficiencia. En este sentido, en la siguiente sección, se explorará que es lo que ocurre cuando entra en contexto el tamaño de los en las aplicaciones de aprendizaje a gran escala.

\section{El efecto de la escala}

Partiendo de la definición  \ref{eq:pac_aprendible} y con tal de investigar cómo es que el tamaño de los datos afecta la capacidad de un algoritmo para aprender de manera empírica, nos concentraremos en explorar el valor esperado de la diferencia entre $\hat{R(h_n)}$ y $R(h)$. Suponiendo, cómo es de esperarse en la práctica, que el tamaño de nuestra muestra es $0< n < n^{*}$. De esta manera, siguiendo los pasos de  \cite{BOUSQUET}, podemos descomponer el valor esperado de la siguiente manera:

\begin{equation*}
  \begin{split}
    \mathbf{E}[\hat{R(h_{n})} - R(h)] & = \mathbf{E}[\hat{R(h_{n^{*}})} - R(h)] + \mathbf{E}[\hat{R(h_{n^{*}})} - R(h_n)]\\
                                     & = Err_{app} + Err_{est}
  \end{split}
\end{equation*}

El primer error, $Err_{app}$ proviene del \emph{tamaño} de la familia de hipótesis $H$. Entre más \emph{grande} sea la familia de hipótesis $H$\footnote{Ver Appendice A para definición de dimensión VC}, la probabilidad de que $\hat{R(h_{n^{*}})}$ y $R(h)$ difieran será baja y por lo tanto el valor esperado de este error será pequeño. De hecho, el orden de decrecimiento del error  está dado por la siguiente desigualdad\cite{VAPNIK2} con una probabilidad de almenos $1-\eta;\eta >0$:

\begin{equation*}
  \begin{split}
    \displaystyle_{h \in H} |R(h) - R_{n^{*}}(h)| & \leq O\bigg( \sqrt{\frac{1}{2n}log(\frac{2}{\eta}) + \frac{d_H}{n}log(\frac{n}{d_H})}\bigg)
  \end{split}
\end{equation*}


Por otra parte, el error $Err_{est}$ proviene de la dificultad intrínseca de aproximar $h$ dado un tamaño de muestra $n<n^{*}$. El compromiso existente entre los dos tipos de errores es claro. Si bien por un lado una familia de hipótesis \emph{grande} permitirá disminuir el primer error, el simple aumento en el espacio de búsqueda hará que $h$ sea más dificil de aproximar y por ende el segundo error crecerá.

Ahora bien, es claro que el problema de encontrar $h_n$ que minimice \ref{eq:def_emp_err} es una operación computacionalmente costosa. Tomemos por ejemplo\cite{JUDD} donde $H$ está constituida por redes neuronales \emph{feed-forward}. Ahí se muestra que el problema de \emph{carga de información} a las distintas capas de la red es un problema NP-completo. Por esta razón, con valores grandes del tamaño de muestra $n$, el sistema computacional que lleve la optimización a cabo muy probablemente se verá restringido por un tiempo máximo aceptable de cómputo $T_{max}$. De aquí que en el caso de aprendizaje a gran escala no se pueda aspirar a estimar $h_n$ si no a lo más $h_{opt}$. De aquí se desprende una nueva descomposición del error esperado:

\begin{equation*}
  \begin{split}
    \mathbf{E}[\hat{R(h_{n})} - R(h)] & = \mathbf{E}[\hat{R(h_{n^{*}})} - R(h)] + \mathbf{E}[\hat{R(h_{n^{*}})} - R(h_n)] +  \mathbf{E}[\hat{R(h_{opt})} - R(h_n)]\\
                                     & = Err_{app} + Err_{est} + Err_{opt}
  \end{split}
\end{equation*}

Esta ecuación comprende ahora tres variables que se comprometen mutuamente: El tamaño de la familia de hipótesis, el número de observaciones disponibles en la muestra y el tiempo tolerable para el algoritmo de optimización. Surge entonces una disyuntiva antes inexistente, a saber, que tanto utilizar los datos en la muestra $n$ para que el balance entre tamaño de $Err_{opt}$ y el tiempo en el proceso de optimización $T_{max}$ sea satisfactorio. Esto da origen a los algoritmos de optimización que utilizan submuestras aleatorias de los datos disponibles. Estos algoritmos se denominan de optimización estocástica. En la siguiente sección exploraremos las diferentes formas de dichos algoritmos, veremos que difieren en cuanto al tipo de información que utilizan, primer orden o segundo orden. Veremos la cantidad de datos que utilizan en cada iteración, en línea, \emph{mini-batch} o \emph{full-batch}. Además de esto, estudiaremos sus tasas de convergencia y contextos bajo los cuales se puede esperar un buen o mal comportamiento de los mismos. De manera concreta y con tal de aterrizar las ideas, en la siguiente sección y por el resto de este documento, en lugar de enfocarnos en una familia de hipótesis abstracta, supondremos que la familia $H$ está parametrizada por un conjunto de parámetros $w$. De esta forma, el objetivo será analizar el comportamiento de diversos algoritmos que busquen resolver el siguiente problema de opitmización:

\begin{defn}\label{eq:central_prob}
\begin{equation*}
\begin{split}
\displaystyle\min_{w\in W}\hat{R(h_{w,n})}=\frac{1}{n}\displaystyle\sum_{i=i}^n l(h_{w_i,n},c) &; n\leq n^{*}
\end{split}
\end{equation*}

\end{defn}

\chapter{Optimización estocástica}

\epigraph{It’s more important than ever to understand the fundamentals of
algorithms as well as the demands of the application, so that good
choices are made in matching algorithms to applications.}{\textit{Stephen Wright \\ University of Wisconsin-Madison}}
\newpage

\section{Descenso en gradiente estocástico}

En esta sección nos enfocaremos en analizar métodos que utilizan información incompleta, de ahí la parte estocástica, referente al gradiente de la función que se desea minimizar. De manera general, estos métodos hacen uso de una aproximación local de primer orden para encontrar, de manera iterativa, una dirección de descenso que, bajo ciertos supuestos de regularidad \cite{NOCEDAL}, eventualmente llegará a un mínimo local\footnote{En este trabajo nos enfocaremos en optimización local. Para un recuento extensivo de métodos de optimización global, el lector puede referirse a \cite{TORN}}. Todos estos métodos tienen, en principio, la misma \emph{forma} que el algoritmo propuesto por \cite{ROBBINS}. Por esta razón, al igual que \cite{BOTTOU}, nos referiremos a todos los algorimos expuestos en esta sección y en lo que queda de la tesis como métodos de \emph{Descenso en Gradiente Estocástico (DGE)}. 

Esta sección seguirá muy de cerca las líneas de \cite{BOTTOU} y , con tal de minimizar \ref{eq:central_prob}, el algoritmo de \emph{Descenso en Gradiente Estocástico (DGE)} se define de la siguiente manera:

%\begin{algorithm}[H]
%\caption{DGE}\label{dge}
%\begin{algorithmic}[1]
%\Procedure{DGE}{}
%\State $\textit{w_0} \gets \text{punto inicial}$
%\BState \emph{top}:
%\State $j \gets \textit{patlen}$
%\BState \emph{loop}:
%\State $j \gets j-1$.
%\State $i \gets i-1$.
%\State \textbf{goto} \emph{loop}.
%\State \textbf{close};
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

En este análisis, $g(w_k, \zeta_k)$ tiene la siguiente forma:

Cómo puede apreciarse fácilmente, esta definición nos da la flexibilidad de optar por un descenso de tipo \emph{batch} u \emph{online}. Nos permite además hacer el análisis de métodos que tomen en cuenta exclusivamente información de primer orden, haciendo $H_k = I_{nxn}$ o que incluyan información de segundo orden. Ya sea de manera exacta, haciendo $H^{-1}_k = \nabla^2_{ww}\hat{R(h_{w,n})}$ obteniendo así la dirección de descenso de Newton o con una aproximación a esta última, utilizando un pseudo Newton.

Ahora bien, para llevar a cabo el análisis de la velocidad de convergencia y determinar los compromisos que deben hacerse para seleccionar un método \emph{batch} sobre un \emph{en línea} es necesario hacer dos supuestos adicionales. 

\subsection{Más allá del primer orden}


\chapter{Dos algoritmos estocásticos de segundo orden}

\epigraph{You're going to see speech recognition systems that have human or better-than-human accuracy become commercialized.}{\textit{Tim Tuttle \\ Massachusetts Institute of Technology}}

\newpage

\section{Newton truncado con gradiente conjugado}

\section{SLM-LBFGS}

\chapter{Implementación y resultados}

\chapter{Conclusión}


\begin{thebibliography}{9}
\bibitem{BENNETT} Kristin P. Bennett, Emilio Parrado-Hernández, \emph{The Interplay of Optimization and Machine Learning Research}. Journal of Machine Learning Research, 2007.
\bibitem{BERNOULLI}Bernoulli, Jakob. \emph{Ars Conjectandi, opus posthumum.} (1795).
\bibitem{BOTTOU}Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. \emph{Optimization Methods for Large-Scale Machine Learning}. arXiv preprint arXiv:1606.04838 (2016).
  \bibitem{BOTTOU2}Bottou, Léon. \emph{Large-scale machine learning with stochastic gradient descent}. Proceedings of COMPSTAT'2010. Physica-Verlag HD, 2010. 177-186.
\bibitem{BOUSQUET}Bousquet, Olivier, and Léon Bottou. \emph{The tradeoffs of large scale learning}. Advances in neural information processing systems. 2008.
\bibitem{BYRD} Richard, H. Byrd, Gillian M. Chin, Will Neveitt \& Jorge Nocedal.
  \emph{On the Use of Stochastic Hessian Information in Optimization Methdos for Machine Learning.} Society of Industrial and Applied Mathematics, 2011.
\bibitem{EFRON} Bradly Efron and Carl Morris, \emph{Stein's Paradox in Statistics}. Scientific American, Volume 236, Issue 5.
\bibitem{HASTIE} Hastie, T.; Tibshirani, R. \& Friedman, J. (2001), \emph{The Elements of Statistical Learning}, Springer New York Inc. , New York, NY, USA .
\bibitem{MASSART} P. Massart, \emph{The tight constant in the dvoretzky kiefer wolfowitz inequality}. The Annals of Probability, 1990, Vol 18, No 3. pg 1269-1283. 
\bibitem{JUDD}Judd, Stephen. \emph{On the complexity of loading shallow neural networks.} Journal of Complexity 4.3 (1988): 177-192.
\bibitem{MAYER} Mayer-Schönberger, Viktor, and Kenneth Cukier. \emph{Big data: A revolution that will transform how we live, work, and think}. Houghton Mifflin Harcourt, 2013.
\bibitem{NOCEDAL} J. Nocedal and S. J. Wright. \emph{Numerical Optimization}, 2nd ed., Springer Ser. Oper. Res., Springer, New York, 2006.
\bibitem{SHAI} Shai Shalev-Shwartz, Shai Ben-David. \emph{Understanding Machine Learning, From Theory to Algorithms},Cambridge University Press, New York, 2014.
\bibitem{STEIN1} Charles Stein. \emph{Inadmissibility of the usual estimator for the mean of a multivariate normal distribution}, Proceedings of the Third Berkley Symposium on Mathematical Statistics and Probability, Berkley and Los Angeles, University of California Press 1956 Vol 1. pp 197-206
\bibitem{STEIN2} W. James and Charles Stein. \emph{Estimation With Quadratic Loss},  Proceedings of the Fourth Berkley Symposium on Mathematical Statistics and Probability, Berkley and Los Angeles, University of California Press 1961. pp 361-379.
\bibitem{TORN} Törn, Aimo, Montaz M. Ali, and Sami Viitanen. \emph{Stochastic global optimization: Problem classes and solution techniques}. Journal of Global Optimization 14.4 (1999): 437-447.
\bibitem{TUKEY} Tukey, J.W., \emph{A survey of sampling from contaminated distributions.} (1960a), Chapter 39 in: \emph{Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling} (ed. I. Olkin et al.), Stanford University Press, Stanford, California, 448-485.
\bibitem{VALIANT} Valiant, Leslie G. \emph{A theory of the learnable}. Communications of the ACM 27.11 (1984): 1134-1142.
\bibitem{VAPNIK1} Vladimir N. Vapnik  \emph{Statistical Learning Theory},JOHN WILEY \& SONS, INC., USA, 1998.
\bibitem{VAPNIK2} Vladimir N. Vapnik  \emph{The Nature of Statistical Learning Theory},Springer, Second Edition, 1999.
\bibitem{WALLER} Waller, Matthew A., and Stanley E. Fawcett. \emph{Data science, predictive analytics, and big data: a revolution that will transform supply chain design and management}." Journal of Business Logistics 34.2 (2013): 77-84.

\end{thebibliography}

\end{document}
